{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c348236",
   "metadata": {},
   "source": [
    "# Neural networks for audio classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a3d82f-569b-416a-9a07-849a9f6b6f21",
   "metadata": {},
   "source": [
    "## Part 2: Dataset preparation\n",
    "\n",
    "We will apply the pipeline (called augment audio) to our raw audio data now, to prepare them for the training of our neural networks. \n",
    "\n",
    "To preprocess the data we will define an object called **data loader**. It can be used to load a dataset and apply various processing steps before returning it as a whole or in small batches. We will use it mainly for the first purpose here. \n",
    "\n",
    "We take two steps:\n",
    "1. The first is loading the audio signals (.wav files) from the hard drive. This will be done for the complete dataset and saved in memory (inside the dataloader). \n",
    "2. The second is converting the signals into mfcc features. We have done this with single examples already in the first exercise.\n",
    "\n",
    "Comment: \n",
    "It is possible to do all this without a dataloader class by directly applying a function to a pre loaded dataset. However, using it here might help you understand its usage in other procects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aebd43dd-f6b2-487e-beb2-351b955b2626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from config import *\n",
    "\n",
    "## Load the dataframe\n",
    "df_all = pd.read_pickle(data_dir + 'df_all_balanced_split.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a05e641-9dc3-4c74-9196-3a2d27b8a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split the dataset\n",
    "df_train = df_all[(df_all.dataset == 'training')]\n",
    "df_val   = df_all[df_all.dataset == 'validation']\n",
    "df_test  = df_all[df_all.dataset == 'testing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba3099f7-af62-42e1-9818-e9aa4226fbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import keep_only_n_unknowns, pad_signal, augment_audio, get_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d58ab994-5b1e-46f8-89c8-be58d534088a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a0ce38d2ff44d9a61184dd2c691b8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/37250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11395b1ef63c4ef7b71ee84cb9b3c5ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4474 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7340260055d04d628648d09edfa42f5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4914 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## training hyperparameters\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tensorflow import keras\n",
    "from tqdm.auto import tqdm  \n",
    "import librosa\n",
    "\n",
    "## activate tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "## fix random seeds for tensorflow\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "## load the raw audio data into memory\n",
    "signals_train = df_train.file_path.progress_apply(lambda x: pad_signal(librosa.load(data_dir + x, sr=fs)[0],\n",
    "                                                                    fs)).values\n",
    "signals_val   = df_val.file_path.progress_apply(  lambda x: pad_signal(librosa.load(data_dir + x, sr=fs)[0],\n",
    "                                                                    fs)).values\n",
    "signals_test  = df_test.file_path.progress_apply( lambda x: pad_signal(librosa.load(data_dir + x, sr=fs)[0],\n",
    "                                                                    fs)).values\n",
    "\n",
    "## loading the labels\n",
    "keywords_test  = df_test.label_one_hot.apply(lambda x: np.asarray(x).astype('float32')).values\n",
    "keywords_val   = df_val.label_one_hot.apply(lambda x: np.asarray(x).astype('float32')).values\n",
    "keywords_train = df_train.label_one_hot.apply(lambda x: np.asarray(x).astype('float32')).values\n",
    "\n",
    "## we need to treat silence utterances differently, so we need to pass the silence label to the loader\n",
    "silence_label = df_all[df_all.keyword == 'silence'].label_one_hot.iloc[0]#.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25c1b5a0-3f53-4947-af1e-07de58079da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## small utility for saving\n",
    "import os \n",
    "\n",
    "def save_dataset(dataset, name=\"\"):\n",
    "    \n",
    "    if not os.path.isfile(data_dir + \"X_\"+name+\".npy\"):\n",
    "        np.save(data_dir + \"X_\"+name, dataset[0], allow_pickle=False)\n",
    "    else:\n",
    "        print(\"file already exists!\")\n",
    "        \n",
    "    if not os.path.isfile(data_dir + \"Y_\"+name+\".npy\"):\n",
    "        np.save(data_dir + \"Y_\"+name, dataset[1], allow_pickle=False)\n",
    "    else:\n",
    "        print(\"file already exists!\")\n",
    "    \n",
    "    print(\"saved dataset with name:\", name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058f7467-aacb-4e42-8cc5-e38f382bc251",
   "metadata": {},
   "source": [
    "## Define various loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "357caf72-20b1-4975-b257-e9e63b2ee9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a loader that calculates mfccs and provides batches of data, especially important later\n",
    "class GSCLoader(tf.keras.utils.Sequence):\n",
    "    ''' Loader provides batches of size batchsize with features x' and labels y where x' = f(x) '''\n",
    "    \n",
    "    def __init__(self, batchsize, x, y, f=None, silence_label=None):\n",
    "        \n",
    "        self.x = np.stack(x)\n",
    "        self.y = np.stack(y)\n",
    "        self.batchsize = batchsize\n",
    "        self.indices   = np.arange(self.x.shape[0])\n",
    "        self.f         = f\n",
    "        self.silence_label = np.argmax(silence_label)\n",
    "    \n",
    "    ## return the number of batches per epoch\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.x) / self.batchsize))\n",
    "\n",
    "    ## return a batch of features, labels\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        inds = self.indices[idx * self.batchsize:(idx + 1) * self.batchsize]\n",
    "        features = np.array([self.f(silence=np.argmax(self.y[i]==self.silence_label), sig=self.x[i]) for i in inds])\n",
    "        labels = np.array(self.y[inds])\n",
    "        \n",
    "        return features , labels\n",
    "\n",
    "    ## shuffle the training data when done with one epoch\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "        print('shuffling indices') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76aac7b-7d4c-442a-b23e-63abd62bc868",
   "metadata": {},
   "source": [
    "### Train loader without data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0ceaf3d",
   "metadata": {
    "id": "herbal-gentleman"
   },
   "outputs": [],
   "source": [
    "## Define the function to calculate mfccs from the audio signal\n",
    "def f(silence, sig):\n",
    "    return augment_audio(silence, mode = '', sig=sig, fs=fs, l=l, s=s, n_mfccs=n_mfccs, padd_audio_to_samples=fs)\n",
    "    \n",
    "## Create the loaders with a batchsize that returns the whole dataset when the loader is called\n",
    "train_loader = GSCLoader(f = f, batchsize = len(keywords_train), y = keywords_train, x = signals_train, \n",
    "                         silence_label=silence_label)\n",
    "val_loader   = GSCLoader(f = f        , batchsize = len(keywords_val) , y = keywords_val,   x = signals_val, \n",
    "                         silence_label=silence_label)\n",
    "test_loader  = GSCLoader(f = f        , batchsize = len(keywords_test), y = keywords_test,  x = signals_test, \n",
    "                         silence_label=silence_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f999804-53a0-4df8-a287-ad5efb49a353",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "## Validation set\n",
    "val_data = val_loader.__getitem__(0)\n",
    "save_dataset(val_data, name=\"val_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77cc7429-787c-4722-b45a-15731ae31d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "## Validation set\n",
    "val_data = train_loader.__getitem__(0)\n",
    "save_dataset(val_data, name=\"train_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b468cdda-14df-4e11-9fb6-0eb044c20546",
   "metadata": {},
   "source": [
    "### Train loader with augmentation (Just execude it for now, we will come to it later!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c175f28-3cf3-4b2c-9c73-05a8d0939c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set augmentation criteria\n",
    "time_shift_by_max = 0.1  # randomized time shift [s]\n",
    "background_frequency = 0.8  # how often is background folded in? 1 = always, 0 = never\n",
    "Ab= 0.1  # background amplitude\n",
    "Nfold = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c97a9dc-109a-40b4-9e9a-e762dffd9258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found # of files: 6\n"
     ]
    }
   ],
   "source": [
    "def f_augment(silence, sig):\n",
    "    mode = 'training'\n",
    "    return augment_audio(silence, mode, sig, fs=fs, \n",
    "                              time_shift_by_max=time_shift_by_max,\n",
    "                              background_frequency=background_frequency,\n",
    "                              noise_data=noise_data,\n",
    "                              Ab=Ab,\n",
    "                              l=l, s=s, n_mfccs=n_mfccs, \n",
    "                              padd_audio_to_samples=fs)\n",
    "\n",
    "## For the training set signals will be augmented aka mode=='training'\n",
    "from utility import load_all_wavs_in_dir\n",
    "noise_data = load_all_wavs_in_dir(direc=brn_directory, sr=fs)\n",
    "\n",
    "## create a train loader that again returns the whole dataset in one batch, but applies f_augment this time\n",
    "train_loader_augmented = GSCLoader(f = f_augment, batchsize = len(keywords_train), y = keywords_train, x = signals_train, \n",
    "                         silence_label=silence_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f37ac83-054c-4735-908b-7692fc1c6b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62d84ce087704493926e2a5a95ade692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%capture --no-display\n",
    "## create an nfold training set. X,Y will be the baseline (1 fold) and X_train, Y_train Nfold\n",
    "X_train_nf, Y_train_nf = train_loader_augmented.__getitem__(0)\n",
    "\n",
    "for i in tqdm(range(Nfold-1)):\n",
    "    X,Y = train_loader_augmented.__getitem__(0)\n",
    "    X_train_nf = np.append(X_train_nf, X, axis=0)\n",
    "    Y_train_nf = np.append(Y_train_nf, Y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7bd9741-400f-4e93-95c1-b043eb420dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "save_dataset([X,Y], name=\"train_data_augmented\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bd1e8e3-1d39-42d9-aaa0-38e00609d0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "save_dataset([X_train_nf,Y_train_nf], name=\"train_data_augmented_nfold\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "8aab069b"
   ],
   "name": "Kopie von 00_all_in_one_kws.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
