{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21eaabfa",
   "metadata": {},
   "source": [
    "# Part 1: Audio preprocessing (.wav to mfcc)\n",
    "\n",
    "This notebook discusses the methods for processing **audio files** (speech in our case) to **MFCC** features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3917c0",
   "metadata": {},
   "source": [
    "### Importing necessary variables and libraries along with GSC dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247f451e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we load some variables from the config file\n",
    "from config import *\n",
    "\n",
    "## calculate the window length and hopsize in samples from window length and stride [ms]\n",
    "lprime = l * int(fs / 1000)\n",
    "hprime = lprime - s * int(fs / 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d77be",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we download the GSC dataset that we will use later\n",
    "!python3 download_and_prepare_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d181aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "## we load some of our own functions and libraries\n",
    "from utility import load_all_wavs_in_dir\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import random \n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2889705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load background noise files to 'noise_data' for later use\n",
    "noise_data = load_all_wavs_in_dir(direc=brn_directory, sr=16000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e850708",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "We load a recording from the dataset and visualize it as a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7da61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, rate = librosa.load(data_dir + \"yes/0132a06d_nohash_0.wav\", sr=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9768b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can plot it\n",
    "plt.plot(y, alpha=1., color='blue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6a388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen to it as well\n",
    "ipd.Audio(y, rate=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c84d4d4",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "We want to build an audio pipeline for the **keyword spotter**. That means we need a function to transform the **.wav** files of our dataset into **spectrograms** or **mfcc** features. To do that, we will use the **librosa** (https://librosa.org/doc/latest/index.html) library.\n",
    "\n",
    "1. First, derive a formula for the dimensions of the expected number of frames for a window length $l = 40 ms$, a stride $s = 20 ms$ and a duration of the audio signal of $T = 1s$. How many feature vectors will we get from our 1 second long audio signal?\n",
    "\n",
    "\n",
    "2. Use librosa built in functions to firstly load a file from the dataset and secondly calculate the following:\n",
    "- A short time Fourier Transformation,\n",
    "- Calculate a mel spectrogram from it. Take the logarithm for better visualization. \n",
    "- Calculate mfccs from it. \n",
    "\n",
    "For each step, visualize the resulting spectrogram images using `plt.imshow()` \n",
    "\n",
    "3. Compare the results of the Fourier transform and mel spectrogram. What did the mel scaling achieve? Why was this useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b87b0b",
   "metadata": {},
   "source": [
    "## Hints:\n",
    "- Use `librosa.stft`, `librosa.feature.melspectrogram`, `librosa.feature.mfcc`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6821439",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4047d459",
   "metadata": {
    "id": "ff888c58"
   },
   "source": [
    "### E1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f63b5e",
   "metadata": {
    "id": "564e6a31"
   },
   "source": [
    "The function augments the input signal and maybe adds background noise, shifts audio etc.\n",
    "The audio features are mfccs and are calculated with `librosa.feature.mfcc`, hopsize $h$ is passed as **parameter**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729db7c8",
   "metadata": {
    "id": "96bfbac1"
   },
   "source": [
    "The hopsize $H$ in $ms$ is given by:\n",
    "\\begin{equation}\n",
    "H  = L-S = 20\\ ms\n",
    "\\end{equation}\n",
    "\n",
    "The hopsize $h$ in samples, is:\n",
    "\\begin{equation}\n",
    "h = H  \\ f_s = 320\n",
    "\\end{equation}\n",
    "\n",
    "The number of frames $N_f$ is: \n",
    "\\begin{equation}\n",
    "N_f = [\\frac{N_s - s}{h}]   = 49\n",
    "\\end{equation}\n",
    "where $N_s$ is the number of samples in the $1s$ audio signal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8192fe4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TyftDxk9M623",
    "outputId": "33919d62-6334-4463-eb78-1fd8d8248a49"
   },
   "outputs": [],
   "source": [
    "(16000 - 320)/320 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfab0033",
   "metadata": {},
   "source": [
    "### E2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae518b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the abs value of frequencies at time t\n",
    "STFT_abs = np.abs(librosa.stft(y, n_fft=lprime, hop_length=hprime, win_length=lprime, center=False))\n",
    "\n",
    "# Plot. Take the log of the values for better visibility\n",
    "plt.imshow(np.log(STFT_abs**2), origin='lower', aspect='auto')\n",
    "# plt.imshow(STFT_abs, origin='lower', aspect='auto')\n",
    "\n",
    "plt.ylabel('frequency bin')\n",
    "plt.xlabel('window number' )\n",
    "plt.colorbar()\n",
    "print('its hard to see the interesting, low frequency stuff!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec3d3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Rescale this to the mel scale and take log of the values for better visibility\n",
    "MEL = librosa.feature.melspectrogram(S=STFT_abs, sr=fs, power=1, n_mels=100)\n",
    "\n",
    "plt.imshow(np.log(MEL), origin='lower', aspect='auto')\n",
    "plt.ylabel('log mel frequency bin')\n",
    "plt.xlabel('window number' )\n",
    "plt.colorbar()\n",
    "print('now we see it better!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deefab4",
   "metadata": {},
   "source": [
    "The mel scaling emphasized the lower frequency parts of the spectrum. In the example we can nicely see the vowel $e$ from the word $yes$ as horizontal bands of fixed frequency. That helps for speech recognition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0761bced",
   "metadata": {},
   "outputs": [],
   "source": [
    "MFCC = librosa.feature.mfcc(S=np.log(MEL))\n",
    "plt.imshow(MFCC, origin='lower', aspect='auto')\n",
    "plt.ylabel('mfcc number')\n",
    "plt.xlabel('window number' )\n",
    "plt.colorbar()\n",
    "print('finally convert the spectrogram to mfcc features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b089a8",
   "metadata": {},
   "source": [
    "# Part 2: Manipulating audio\n",
    "\n",
    "Now, we know how to calculate mfcc features and have a feeling for what is going on when looking at spectrograms. In the next part, we will familiarize ourselves a bit more with audio signals by shifting them in time and adding background noise.\n",
    "\n",
    "### Augmenting audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaae483",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utility import augment_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939d85f3",
   "metadata": {
    "id": "21a45e99"
   },
   "outputs": [],
   "source": [
    "## Load a sample audio file\n",
    "y, rate = librosa.load(data_dir + \"yes/0132a06d_nohash_0.wav\", sr=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5d5937",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 61
    },
    "id": "dd239105",
    "outputId": "d2e95e22-9340-4472-8e3d-0b9356179314"
   },
   "outputs": [],
   "source": [
    "## Function will add background noise to the audio signal:\n",
    "y_augmented_noise = augment_audio(sig=y, # the signal\n",
    "                            fs=fs, # sample frequency of the signal\n",
    "                            padd_audio_to_samples=fs, # audio length is adjusted to this value\n",
    "                            noise_data = noise_data, # use these noise data\n",
    "                            background_frequency=1., # 1. means always add background noise\n",
    "                            Ab=0.3, # with amplitude Ab\n",
    "                            n_mfccs=0 # 0 --> get the signal directly\n",
    "                           )\n",
    "\n",
    "ipd.Audio(y_augmented_noise, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9904329d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Also, calculate mfcc features\n",
    "mfcc_augmented = augment_audio(silence=False, mode='training',\n",
    "                  sig=y, fs=16000, padd_audio_to_samples=16000,\n",
    "                  background_frequency=1., noise_data=noise_data, Ab=0.3,\n",
    "                  l=40, s=20, n_mfccs=16)\n",
    "\n",
    "mfcc_clean = augment_audio(silence=False, mode='training',\n",
    "                  sig=y, fs=16000, padd_audio_to_samples=16000,\n",
    "                  background_frequency=0., noise_data=noise_data, Ab=0.,\n",
    "                  l=40, s=20, n_mfccs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9014415c",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "1. Implement a function called **shift** that shifts the audio signal by $ts$ seconds.\n",
    "\n",
    "```\n",
    "def shift(y, ts, fs):\n",
    "    \n",
    "    ## shift y\n",
    "    if ts > 0.:\n",
    "        \n",
    "    elif ts < 0.:\n",
    "    \n",
    "    else:\n",
    "        y_shifted = y\n",
    "    \n",
    "    return y_shifted\n",
    "    \n",
    "```\n",
    "\n",
    "2. Take the signal $y$ from above and shift it. Compare the original and the shifted signals in a plot and with ipd.Audio. Does the method work?\n",
    "\n",
    "3. Above we have already created two different mfcc versions of the recording **mfcc_augmented** and **mfcc_clean**. Plot them and compare. What did the addition of background noise do in the result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67388a2f",
   "metadata": {},
   "source": [
    "## Hints\n",
    "- Use `numpy.clip()`.\n",
    "- For comparing the mfccs, use `fig,axs = plt.subplots(3,)` to create three plots and `axs[i].plot` and`axs[i].imshow()` to display the raw audio signal and the mfccs respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88315277",
   "metadata": {},
   "source": [
    "### E1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0749ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shift(y, ts, fs):\n",
    "    '''Randomly time shifts a signal y with sample frequency fs by up to +- ts'''\n",
    "    \n",
    "    ## Copy the signal\n",
    "    y_shift = np.copy(y)\n",
    "    \n",
    "    shift_in_samples = int(np.abs(ts * fs))\n",
    "    \n",
    "    ## Perform the shift (different for ts>0, ts<0)\n",
    "    if ts > 0:\n",
    "        ## Add 0s to the left and take original signal length starting from sample 0:\n",
    "        y_shift = np.pad(y_shift, (shift_in_samples, 0), mode='constant')[0:y_shift.shape[0]]\n",
    "    elif ts < 0:\n",
    "        ## Add 0s to the right and take original signal length starting from shift in samples:\n",
    "        y_shift = np.pad(y_shift, (0, shift_in_samples), mode='constant')[\n",
    "                  shift_in_samples:y_shift.shape[0] + shift_in_samples]\n",
    "\n",
    "    return y_shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a181c1",
   "metadata": {
    "id": "34f19b28"
   },
   "source": [
    "### E2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be7d99",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313
    },
    "id": "f20d06a1",
    "outputId": "6a6f6834-dc59-4c52-c2c3-fb2b813cba41"
   },
   "outputs": [],
   "source": [
    "## Plot shifted to see if it works\n",
    "plt.plot(y, label=\"original\")\n",
    "y_shifted = shift(y=y, ts=0.2, fs=rate)\n",
    "plt.plot(y_shifted, label=\"shifted\")\n",
    "plt.legend()\n",
    "\n",
    "ipd.Audio(y, rate = rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c538db",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 61
    },
    "id": "e69c8afd",
    "outputId": "ea1effab-c6d5-45d8-bf0e-9f1b754b26e3"
   },
   "outputs": [],
   "source": [
    "## Listen to shifted audio \n",
    "ipd.Audio(y_shifted, rate = rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ae4f88",
   "metadata": {
    "id": "b3c7018b"
   },
   "source": [
    "### E3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c50321",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 681
    },
    "id": "e6082e6f",
    "outputId": "d5443085-887c-4991-be62-c232e449ac68"
   },
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(3,)\n",
    "fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "axs[0].plot(y, color='blue')\n",
    "axs[0].plot(y_augmented_noise, color='red', alpha=0.3)\n",
    "\n",
    "\n",
    "i1 = axs[1].imshow(mfcc_clean.T, origin='lower', aspect='auto', label='clean')\n",
    "\n",
    "i2 = axs[2].imshow(mfcc_augmented.T, origin='lower', aspect='auto', label='augmented')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c140fbd",
   "metadata": {
    "id": "ceaa1e48"
   },
   "source": [
    "### Comparison:\n",
    "- Additive noise can be seen in the waveform.\n",
    "- If noise comes from multiple (roughly equally distributed) frequency channels it can be seen in all mfcc coefficients."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "8aab069b"
   ],
   "name": "Kopie von 00_all_in_one_kws.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
