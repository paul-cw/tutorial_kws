{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks for audio classification\n",
    "\n",
    "## Part 1: Dataset inspection\n",
    "\n",
    "The first step is always visualising our data. We have ignored this for the sake of having more time for audio processing so far. We will load a dataframe that contains metadata about our dataset as well as the file paths and investigate it in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For GPUs,**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Activate gpu usage if available\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:    \n",
    "    try:  \n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "else:\n",
    "    print('no gpus found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "herbal-gentleman"
   },
   "outputs": [],
   "source": [
    "from config import *\n",
    "import pandas as pd\n",
    "from utility import keep_only_n_unknowns, pad_signal, augment_audio, get_callbacks\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the dataframe\n",
    "df_all = pd.read_pickle(data_dir + 'df_all.pkl')\n",
    "df_all.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2c4a3264"
   },
   "source": [
    "## Exercise 1\n",
    "1. Visualize the dataset. How are the recordings distributed in terms of **\"keyword\"** and **\"speaker_id\"**? Are there many different speakers?\n",
    "2. Would you adjust the class distribution? Set the \"balance_out\" flag to **True** or **False**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "- Useful commands **-** df.describe, df['column'].value_counts\n",
    "- Single columns can be selected by passing their name as a string: df['name']\n",
    "- Columns of dataframes can be selected by passing a list of strings: df[name_list]\n",
    "- A pandas series object (column of a data frame) has a **\"plot\"** method that can be helpful, **Use:**  `.plot(kind='bar')`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0c4efc76"
   },
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1 \n",
    "1. The **\"describe\"** command below shows, that we have something like $2500$ unique speakers with roughly a maximum of $25$ (speak_ut unique value) utterances per keyword each. The **freq** column also tells us that our top recording speaker has $232$ recordings over all keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "efd37ed6",
    "outputId": "cd139470-f9b8-4052-b351-9b4e311eba03"
   },
   "outputs": [],
   "source": [
    "## select the columns of interest and print some statistics\n",
    "df_all[['keyword', 'speaker_id', 'speaker_ut']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "id": "b57dd25f",
    "outputId": "df82cf9c-4215-4c0e-e7fc-9d29a26fcea5"
   },
   "outputs": [],
   "source": [
    "## for all speakers, plot the keyword distribution as histogram\n",
    "df_all['keyword'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E2\n",
    "We can see from the bar chart that we have many examples of the **unknown** keyword and the other classes are well balanced. Well balanced datasets are always preferable since the model training might be influenced by the inbalanced data distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balance_out = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8e963f7"
   },
   "source": [
    "## Part 2: Data loading\n",
    "\n",
    "We need to set up a pipeline that loads the data into memory and provides it to Keras `model.fit()` function that will later perform the training. But first we will split our dataset into $3$ distinct sets. This will be useful for training later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "The dataset has already been split up for us into train, test and evaluation set. We will train the model on the training set and evaluate its performance on the evaluation set later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Balance out the dataset\n",
    "if balance_out:\n",
    "    df_all = keep_only_n_unknowns(df_all, 10)\n",
    "    df_all.keyword.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all['keyword'].value_counts().plot(kind='bar');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split the dataset\n",
    "df_train = df_all[(df_all.dataset == 'training')]\n",
    "df_val   = df_all[df_all.dataset == 'validation']\n",
    "df_test  = df_all[df_all.dataset == 'testing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data loader\n",
    "\n",
    "In anticipation of what's coming later, we will use a data loader. It is an object which can be called by the `model.fit()` method and returns the dataset in batches. We will load the data in two stages. The first is loading the audio signals (.wav files) from the hard drive. This will be done for the complete dataset and saved in memory (inside the dataloader). The second is converting the signals into mfcc features. We will see later in detail why this makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awful-reach"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tensorflow import keras\n",
    "from tqdm.auto import tqdm  \n",
    "import librosa\n",
    "\n",
    "## activate tqdm for pandas\n",
    "tqdm.pandas()\n",
    "\n",
    "## fix random seeds for tensorflow\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training hyperparameters\n",
    "batch_size= 32 # size of the batches for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the raw audio data into memory\n",
    "signals_train = df_train.file_path.progress_apply(lambda x: pad_signal(librosa.load(data_dir + x, sr=fs)[0],\n",
    "                                                                    fs)).values\n",
    "signals_val   = df_val.file_path.progress_apply(  lambda x: pad_signal(librosa.load(data_dir + x, sr=fs)[0],\n",
    "                                                                    fs)).values\n",
    "signals_test  = df_test.file_path.progress_apply( lambda x: pad_signal(librosa.load(data_dir + x, sr=fs)[0],\n",
    "                                                                    fs)).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading the labels\n",
    "keywords_test  = df_test.label_one_hot.apply(lambda x: np.asarray(x).astype('float32')).values\n",
    "keywords_val   = df_val.label_one_hot.apply(lambda x: np.asarray(x).astype('float32')).values\n",
    "keywords_train = df_train.label_one_hot.apply(lambda x: np.asarray(x).astype('float32')).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## we need to treat silence utterances differently, so we need to pass the silence label to the loader\n",
    "silence_label = df_all[df_all.keyword == 'silence'].label_one_hot.iloc[0]#.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a loader that calculates mfccs and provides batches of data, especially important later\n",
    "class GSCLoader(tf.keras.utils.Sequence):\n",
    "    ''' Loader provides batches of size batchsize with features x' and labels y where x' = f(x) '''\n",
    "    \n",
    "    def __init__(self, batchsize, x, y, f=None, silence_label=None):\n",
    "        \n",
    "        self.x = np.stack(x)\n",
    "        self.y = np.stack(y)\n",
    "        self.batchsize = batchsize\n",
    "        self.indices   = np.arange(self.x.shape[0])\n",
    "        self.f         = f\n",
    "        self.silence_label = np.argmax(silence_label)\n",
    "    \n",
    "    ## return the number of batches per epoch\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.x) / self.batchsize))\n",
    "\n",
    "    ## return a batch of features, labels\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        inds = self.indices[idx * self.batchsize:(idx + 1) * self.batchsize]\n",
    "        features = np.array([self.f(silence=np.argmax(self.y[i]==self.silence_label), sig=self.x[i]) for i in inds])\n",
    "        labels = np.array(self.y[inds])\n",
    "        \n",
    "        return features , labels\n",
    "\n",
    "    ## shuffle the training data when done with one epoch\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.indices)\n",
    "        print('shuffling indices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the function to calculate mfccs from the audio signal\n",
    "def f(silence, sig):\n",
    "    return augment_audio(silence, mode = '', sig=sig, fs=fs, l=l, s=s, n_mfccs=n_mfccs, padd_audio_to_samples=fs)\n",
    "    \n",
    "    \n",
    "## Create the loaders with a batchsize that returns the whole dataset when the loader is called\n",
    "train_loader = GSCLoader(f = f, batchsize = len(keywords_train), y = keywords_train, x = signals_train, \n",
    "                         silence_label=silence_label)\n",
    "val_loader   = GSCLoader(f = f        , batchsize = len(keywords_val) , y = keywords_val,   x = signals_val, \n",
    "                         silence_label=silence_label)\n",
    "test_loader  = GSCLoader(f = f        , batchsize = len(keywords_test), y = keywords_test,  x = signals_test, \n",
    "                         silence_label=silence_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dd45f009"
   },
   "outputs": [],
   "source": [
    "## Validation set\n",
    "val_data = val_loader.__getitem__(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training set\n",
    "train_data = train_loader.__getitem__(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait, what? We have created this dataloader, in the end just create a numpy array called training set? Couldn't we have arrived there without the loader? Yes we could have. For now we will just use the val_data and train_data arrays. The reason for creating the loader will become clear after the next lecture but its more reasonable to have it already prepared now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks\n",
    "\n",
    "We will train a classifier (neural network) that predicts which keyword or class is present from the MFCC features of a one-second long audio clip.\n",
    "\n",
    "## Part 3: Set up a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Infer model size\n",
    "n_max_frames     =  49  # leave this at 49 \n",
    "n_output_neurons = len(df_all.keyword.unique())\n",
    "\n",
    "print('features have the dimension:', n_max_frames, 'x', n_mfccs, 'and output neurons:', n_output_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "We will use `tf.keras.models.Sequential()` and feed a list of layers to it to create our model. \n",
    "\n",
    "1. Create a feed forward network with $2$ hidden layers and **ReLU** activation functions, that has a softmax output layer. you can use `tf.keras.Input()` as the input layer before the Dense hidden layers. Use $64$, $128$ neurons for your **\"Dense\"** layers.\n",
    "\n",
    "2. Check the dimensions and parameters of your model using `model.summary()` and try out the `model.predict` function on a training batch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints:\n",
    "- The input dimension is the dimension of the spectrogram image $(49 * 40)$. A hidden layer only accepts $1$ dimensional input. You could use the **reshape** layer to reshape the input to $1$ dimension. \n",
    "- You can use `np.random.random()` and pass it a tuple of $(batchsize, 49, 40)$ to create a random batch for testing the model with the `model.predict()` function.\n",
    "- The predictions should sum up to $1$ because we have used a **Softmax** layer. You can check it with np.sum(prediction_vector)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1\n",
    "Model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(name='input_layer', shape=(n_max_frames, n_mfccs)),\n",
    "        layers.Reshape((n_max_frames * n_mfccs, ), input_shape=(n_max_frames, n_mfccs)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(n_output_neurons, activation='softmax'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E2\n",
    "Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6a9c4d88",
    "outputId": "46a7a22a-d169-4980-fa96-3920962e30ae"
   },
   "outputs": [],
   "source": [
    "model.summary()\n",
    "model.input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(np.random.random((10,49,40)))[0]\n",
    "print(prediction, '\\n sum:', np.sum(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "painted-kidney"
   },
   "source": [
    "# Part 4: Train the model\n",
    "\n",
    "In this part, we will compile the model by providing loss, metrics and an optimizer. We will use one set of parameters for the following trainings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of epochs to run the training for\n",
    "n_epochs= 30\n",
    "\n",
    "## Early stopping setting\n",
    "patience= 25    \n",
    "\n",
    "## Logging/debugging \n",
    "debugging_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[keras.metrics.CategoricalAccuracy()],\n",
    "              run_eagerly=debugging_mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit with training set unaugmented 1fold\n",
    "Now we finally get to training a model. Keras has everything implemented inside the model class. While it is possible to write a custom training, we can just pass all the options to the `model.fit()` function and it will do everything for us. We need to pass:\n",
    "- Training set as $x$ and $y$.\n",
    "- **steps_per_epoch**, which are the number of batches inside the training set.\n",
    "- **n_epochs**, which is the total number of epochs to train for.\n",
    "- **Shuffle**, which automatically shuffles the dataset after each epoch (we set it to **False** for now for all our trainings).\n",
    "- **validation_data**, which is the validation set. This will only be used to calculate loss and accuracy on itself.\n",
    "- Callbacks, which is a collection of methods that are called throughout the training. We have provided a callback function for you that will write out certain metrics like **confusion matrix**, **roc curve** and so on. You should check if you can find those in your output_dir, sorted by the datetime when the training started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, importlib\n",
    "\n",
    "importlib.reload(sys.modules['utility'])\n",
    "from utility import get_callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=train_data[0], y=train_data[1], \n",
    "                    steps_per_epoch=int(np.floor(len(train_data[0]) / batch_size)),\n",
    "                    epochs=n_epochs, \n",
    "                    callbacks=get_callbacks(output_dir, val_data, model, patience=patience), \n",
    "                    validation_data=val_data, \n",
    "                    shuffle=False)\n",
    "\n",
    "print('max val val_categorical_accuracy', np.max(history.history['val_categorical_accuracy']))\n",
    "\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "painted-kidney"
   },
   "source": [
    "## Exercise\n",
    "1. We should see a significant difference between the training and validation accuracies in the plots above. Why is this the case?\n",
    "\n",
    "2. How do you judge the overall accuracy? Play with the number of hidden layers and the number of neurons in the hidden layers and retrain. Do you get a better result?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "- Whats the difference between validation and train data?\n",
    "- Look at the confusion matrices we have dumped to your data folder. What can you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1\n",
    "The validation data are not used for parameter optimization. The phenomenon we encountered is called overfitting and it can have different causes like **data sparsity**, **outliers**, too many **degrees of freedom** etc. We will learn more about it in the next lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E2\n",
    "The confusion matrices show that some keywords get mixed up a lot. The accuracy tells us how many instances are classified correctly or how many keywords are recognized correctly. <br>\n",
    "<br>\n",
    "**Lets try some deeper architectures**,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(name='input_layer', shape=(n_max_frames, n_mfccs)),\n",
    "        layers.Reshape((n_max_frames * n_mfccs, ), input_shape=(n_max_frames, n_mfccs)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),    \n",
    "        layers.Dense(n_output_neurons, activation='softmax'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[keras.metrics.CategoricalAccuracy()],\n",
    "              run_eagerly=debugging_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=train_data[0], y=train_data[1], \n",
    "                    steps_per_epoch=int(np.floor(len(train_data[0]) / batch_size)),\n",
    "                    epochs=n_epochs, \n",
    "                    callbacks=get_callbacks(output_dir, val_data, model, patience=patience), \n",
    "                    validation_data=val_data, \n",
    "                    shuffle=False)\n",
    "\n",
    "print('max val val_categorical_accuracy', np.max(history.history['val_categorical_accuracy']))\n",
    "\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that improving the network's depth helped to increase the accuracy to $74\\%$, so our model was not powerful enough so far. <br>\n",
    "<br>\n",
    "**Lets try more deeper model**, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential(\n",
    "    [\n",
    "        tf.keras.Input(name='input_layer', shape=(n_max_frames, n_mfccs)),\n",
    "        layers.Reshape((n_max_frames * n_mfccs, ), input_shape=(n_max_frames, n_mfccs)),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),    \n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dense(256, activation='relu'),    \n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(512, activation='relu'),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.Dense(n_output_neurons, activation='softmax'),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[keras.metrics.CategoricalAccuracy()],\n",
    "              run_eagerly=debugging_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=train_data[0], y=train_data[1], \n",
    "                    steps_per_epoch=int(np.floor(len(train_data[0]) / batch_size)),\n",
    "                    epochs=n_epochs, \n",
    "                    callbacks=get_callbacks(output_dir, val_data, model, patience=patience), \n",
    "                    validation_data=val_data, \n",
    "                    shuffle=False)\n",
    "\n",
    "print('max val val_categorical_accuracy', np.max(history.history['val_categorical_accuracy']))\n",
    "\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that a deeper model can help to increase the accuracy, however only up to a point. After that, adding parameters might not help anymore."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we would like over $90\\%$, so we have some room for improvement :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP!\n",
    "The following section is reserved for after we are through with lecture 3. If you got here you are done for now :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Improve the model\n",
    "\n",
    "Our goal in this section is to try out some of the techniques we have learned to improve the model quality.\n",
    "That means several things can be done and it is up to you to try out on your own what works. If you prefer a more structured approach you can strictly follow the exercises. \n",
    "\n",
    "In the following you are provided with a new training set in the form of **features** $X$ and **labels** $Y$. It is constructed using a random timeshift and has mixed in background noise. You can select how many times to repeat the augmentation process with the **Nfold** variable below. Keep in mind that we cannot use too big of a number here, since the dataset still needs to fit into memory. Otherwise we would need to train directly with the generator. Despite being possible, it will take considerably longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set augmentation criteria\n",
    "time_shift_by_max = 0.1  # randomized time shift [s]\n",
    "background_frequency = 0.8  # how often is background folded in? 1 = always, 0 = never\n",
    "Ab= 0.1  # background amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the training set signals will be augmented aka mode=='training'\n",
    "from utility import load_all_wavs_in_dir\n",
    "noise_data = load_all_wavs_in_dir(direc=brn_directory, sr=fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For the training set signals will be augmented aka mode=='training'\n",
    "def f_augment(silence, sig):\n",
    "    mode = 'training'\n",
    "    return augment_audio(silence, mode, sig, fs=fs, \n",
    "                              time_shift_by_max=time_shift_by_max,\n",
    "                              background_frequency=background_frequency,\n",
    "                              noise_data=noise_data,\n",
    "                              Ab=Ab,\n",
    "                              l=l, s=s, n_mfccs=n_mfccs, \n",
    "                              padd_audio_to_samples=fs)\n",
    "    \n",
    "## create a train loader that again returns the whole dataset in one batch, but applies f_augment this time\n",
    "train_loader_augmented = GSCLoader(f = f_augment, batchsize = len(keywords_train), y = keywords_train, x = signals_train, \n",
    "                         silence_label=silence_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create an nfold training set. X,Y will be the baseline (1 fold) and X_train, Y_train Nfold\n",
    "Nfold = 3\n",
    "X_train, Y_train = train_loader_augmented.__getitem__(0)\n",
    "\n",
    "for i in tqdm(range(Nfold-1)):\n",
    "    X,Y = train_loader_augmented.__getitem__(0)\n",
    "    X_train = np.append(X_train, X, axis=0)\n",
    "    Y_train = np.append(Y_train, Y, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## keep X,Y as 1 fold augmented data and X_train, Y_train as Nfold augmented data\n",
    "print(X_train.shape[0] / X.shape[0], 'fold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TC-ResNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup a new model architecture\n",
    "from utility import ResBlock\n",
    "\n",
    "def get_tc_resnet(n_max_frames, n_mfccs, n_output_neurons=12, dropout_rate=0.):\n",
    "\n",
    "    T = n_max_frames \n",
    "    F = n_mfccs \n",
    "\n",
    "    n_channels = [16, 24, 32, 48]\n",
    "\n",
    "    model = tf.keras.models.Sequential(\n",
    "        [\n",
    "            tf.keras.Input(name='input_layer', shape=(T, F, 1)),\n",
    "            layers.Reshape((T, 1, F), input_shape=(T, F, 1,)),\n",
    "            layers.Conv2D(filters=n_channels[0], kernel_size=[3, 1], activation=None, use_bias=False,\n",
    "                          padding='same'),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            ResBlock(n=n_channels[1], s=2),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            ResBlock(n=n_channels[1], s=1),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.GlobalAveragePooling2D(),\n",
    "            layers.Dropout(dropout_rate),\n",
    "            layers.Dense(n_output_neurons, activation='softmax'),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "1. Take some time to review the network structure of the **TC resnet** above. How many parameters does the model have?\n",
    "\n",
    "2. Do a baseline run with the train_data set. You can copy the important parts from above. What is the accuracy?\n",
    "\n",
    "3. Lets try to further improve the accuracy by running with the new dataset. Be carefuful to adjust the steps_per_epoch part to the new length of the dataset. You might also want to scale down the number of epochs since the dataset effectively now contains **Nfold epochs**. Possible things to try out are:\n",
    "    - Use the augmented data X_train, Y_train\n",
    "    - Use the set $X$,$Y$ which are shifted and with background noise but just one fold version of the baseline set\n",
    "    - Add dropout by passing the dropout_rate variable to the tc-resnet\n",
    "4. Discuss your results. Which measures helped?\n",
    "\n",
    "<!-- 5. Bonus: download a model from [keras.applications](https://keras.io/api/applications/mobilenet/#mobilenetv2-function) and train it for some epochs. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1\n",
    "- Reshape layer makes sure that we can apply the kernel over all frequencies simultaneously.\n",
    "- Residual blocks $\\rightarrow$ **resnet** like structure.\n",
    "- Dropout layers are added.\n",
    "- Global pooling reduces the size before the softmax layer.\n",
    "- The softmax layer returns probabilities for the $12$ classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E2 baseline: Fit with training set unaugmented 1fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_tc_resnet(n_max_frames, n_mfccs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[keras.metrics.CategoricalAccuracy()],\n",
    "              run_eagerly=debugging_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=train_data[0], y=train_data[1], \n",
    "                    steps_per_epoch=int(np.floor(len(train_data[0]) / batch_size)),\n",
    "                    epochs=n_epochs, \n",
    "                    callbacks=get_callbacks(output_dir, val_data, model, patience=patience), \n",
    "                    validation_data=val_data, \n",
    "                    shuffle=False)\n",
    "\n",
    "print('max val val_categorical_accuracy', np.max(history.history['val_categorical_accuracy']))\n",
    "\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E3: Fit with training set augmented 1fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_tc_resnet(n_max_frames, n_mfccs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[keras.metrics.CategoricalAccuracy()],\n",
    "              run_eagerly=debugging_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "distinct-garlic",
    "outputId": "c5ba8839-fb17-43c0-c1ba-dd51d177098c"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x=X, y=Y, \n",
    "                    steps_per_epoch=int(np.floor(len(X) / batch_size)),\n",
    "                    epochs=n_epochs, \n",
    "                    callbacks=get_callbacks(output_dir, val_data, model, patience=patience), \n",
    "                    validation_data=val_data, \n",
    "                    shuffle=False)\n",
    "print('max val val_categorical_accuracy', np.max(history.history['val_categorical_accuracy']))\n",
    "\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E3: Fit with nfold data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_tc_resnet(n_max_frames, n_mfccs)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[keras.metrics.CategoricalAccuracy()],\n",
    "              run_eagerly=debugging_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "distinct-garlic",
    "outputId": "c5ba8839-fb17-43c0-c1ba-dd51d177098c"
   },
   "outputs": [],
   "source": [
    "n_epochs_corrected = int(n_epochs / 2)\n",
    "\n",
    "history = model.fit(x=X_train, y=Y_train, \n",
    "                    steps_per_epoch=int(np.floor(len(X_train) / batch_size)),\n",
    "                    epochs=n_epochs_corrected , \n",
    "                    callbacks=get_callbacks(output_dir, val_data, model, patience=patience), \n",
    "                    validation_data=val_data, \n",
    "                    shuffle=False)\n",
    "print('max val val_categorical_accuracy', np.max(history.history['val_categorical_accuracy']))\n",
    "\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E3: Fit with nfold augmented + dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_tc_resnet(n_max_frames, n_mfccs, dropout_rate=0.2)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[keras.metrics.CategoricalAccuracy()],\n",
    "              run_eagerly=debugging_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs_corrected = int(n_epochs)\n",
    "\n",
    "history = model.fit(x=X_train, y=Y_train, \n",
    "                    steps_per_epoch=int(np.floor(len(X_train) / batch_size)),\n",
    "                    epochs=n_epochs_corrected , \n",
    "                    callbacks=get_callbacks(output_dir, val_data, model, patience=patience), \n",
    "                    validation_data=val_data, \n",
    "                    shuffle=False)\n",
    "print('max val val_categorical_accuracy', np.max(history.history['val_categorical_accuracy']))\n",
    "\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E4: Discussion\n",
    "Sorted by accuracy:\n",
    "- 1fold augmented set with timeshift and background noise $\\rightarrow$ $91.8\\%$ \n",
    "- Baseline with unaugmented training set $\\rightarrow$ $92.4\\%$ (Makes sense, since validation set is not augmented!)\n",
    "- 3fold augmented $\\rightarrow$ $93.8\\%$\n",
    "- 3fold augmented with dropout of $0.2$ $\\rightarrow$ $94.5\\%$\n",
    "\n",
    "The difference in accuracy is roughly 3 percentage points. That is, with an already optimized structure for keyword spotting. It is a sizeable effect, which could have been even bigger when starting from a different model architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 6: Train using other pretrained models - MobileNetV2\n",
    "\n",
    "In this section we will use a predefined model **(MobileNetV2)** from [keras.applications](https://keras.io/api/applications/) that is meant for **image classification** and try it out for **keyword spotting**. The model can easily be downloaded via $tf.keras.applications$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.applications.MobileNetV2(\n",
    "    input_shape=(49,40,1),\n",
    "    alpha=1.0,\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    pooling=None,\n",
    "    classes=12,\n",
    "    classifier_activation=\"softmax\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "1. Compile and train the model as done above.\n",
    "2. Compare the results to our previous ones in terms of parameters and accuracy. What can we learn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=[keras.metrics.CategoricalAccuracy()],\n",
    "              run_eagerly=debugging_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs_corrected = int(n_epochs / 2)\n",
    "\n",
    "history = model.fit(x=X_train, y=Y_train, \n",
    "                    steps_per_epoch=int(np.floor(len(X_train) / batch_size)),\n",
    "                    epochs=n_epochs_corrected , \n",
    "                    callbacks=get_callbacks(output_dir, val_data, model, patience=patience), \n",
    "                    validation_data=val_data, \n",
    "                    shuffle=False)\n",
    "print('max val val_categorical_accuracy', np.max(history.history['val_categorical_accuracy']))\n",
    "\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('max val val_categorical_accuracy', np.max(history.history['val_categorical_accuracy']))\n",
    "\n",
    "plt.plot(history.history['categorical_accuracy'])\n",
    "plt.plot(history.history['val_categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E2\n",
    "\n",
    "- Model has more parameters.\n",
    "- Performance is similar.\n",
    "\n",
    "What can we learn?\n",
    "- Loading pre-defined bigger convolutional models is a very good way to start your training and get a baseline accuracy. However it might be necessary and rewarding to tailor the model architecture to your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "8aab069b"
   ],
   "name": "Kopie von 00_all_in_one_kws.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
